# 8.期望  
### 8.0期望概述
关于概率是如何分布在变量所有可能值的，随机变量的分布给我们提供了详细信息。但是通常我们只是想大致了解它们分布在数字线的位置。换句话来说，我们只是想知道分布中心的位置。  
经历过多次“期中”考试的任何学生都知道，“中间”和“中心”这样的词语没有独特的含义。本章是关于随机变量的一种特殊“中心”。  
#### 示例  
```
from datascience import *  
from prob140 import *  
import numpy as np  
import matplotlib.pyplot as plt  
plt.style.use('fivethirtyeight')  
%matplotlib inline
```
### 8.1定义  

随机变量X的期望，可以表示为E(X)，是由X经过概率加权所有可能值的平均值。这可以用两种等效的方式来计算。  
在X的域上：
  
　　　　　　　　　　　　E(X) = \sum_{\text{all }x} xP(X=x) 

在X的范围内：　　

　　　　　　　　　　　　E(X)=∑(all x)xP(X=x)

 
解释说明：  
如果X具有有限多个可能的值，则上式的总和总是很好定义并且是有限的。如果X是有相当多的取值，比如由1,2,3...索引的值，那么我们需要更加小心以确保公式可以得到被明确定义的数值。我们后续将很快涉及到这个问题。现在，我们先假设总和已经被明确定义。  
假设总和定义明确，可以直接表明这两个公式会给出相同的答案。 显示它的一种方法是在所有不同的结果ω中，通过X（ω）的不同值对第一个求和公式中的项进行分组。  
第二个公式通常作为期望的“定义”给出，但第一个公式可以有助于理解期望的特性。 特别是，它表明如果两个随机变量具有相同的分布，那么它们也应该具有相同的期望。  
### 重心
假设X具有以下给出的分布：  
示例：  
```
x = np.arange(1, 6)  #创建array([1, 2, 3, 4, 5])  
probs = make_array(0.15, 0.25, 0.3, 0.2, 0.1)  
example_dist = Table().values(x).probability(probs)  
Plot(example_dist)
```


   
![](https://i.imgur.com/0L64UxZ.png)  
然后通过X的范围上的公式，我们得到E（X）= 2.85。  
```
ev_X = sum(x*probs)  
ev_X
```  
2.8499999999999996  
你也可以调用prob14中的ev函数来计算期望E(X):  
```
example_dist.ev()
```  
2.8500000000000005  
期望我们通常也成为期望值，因此不论是该函数的名称还是我们的名称ev_V，都是可以这么称呼的。但是请注意期望值不一定是随机变量的可能值。例如该随机变量的可能值就不是2.85.  
但那么期望值代表着什么呢？要想看到这一点，我们要使用Plot中的show_ev = True的参数，来可视化期望E(X).  
```Plot(example_dist, show_ev=True) ``` 
![](https://i.imgur.com/nFNFMj9.png)


##  
如果你已经研究了一些物理学，你会发现我们用于期望的公式与系统的重心公式是相同的，其中权重等于从1,2,3,4,5这些可能值相应的概率。  
因此，假设直方图是由纸板或一些刚性材料制成，并想象试图在水平轴上某处固定的铅笔尖上找到平衡点。 你需要将铅笔保持在2.85才能达到平衡。  
期望可以看作是物理意义上的分布中心：它是分布的重心或质心。
### 长期平均成本曲线  
当您在相同条件下一次又一次地生成变量时，您还可以将期望视为随机变量的长期平均值。适用于prob140分配对象的sample_from_dist方法允许您这样做。它随机抽样，从分布中替换，并返回一组采样值。 参数是样本大小。  
您可以使用emp_dist函数将模拟值数组转换为分布对象，可以将其与Plot和其他prob140函数一起使用。 Plot的show_ave = True参数可以显示模拟值的平均值。  
#### 示例  
``` 
simulated_X = example_dist.sample_from_dist(10000)  
emp_dist_X = emp_dist(simulated_X)  
Plot(emp_dist_X, show_ave=True)  
plt.title('Empirical Distribution');  
```
![](https://i.imgur.com/nxIBBxW.png)  

X的10000个模拟值的平均值非常接近E(X)，但不完全相等。  


`np.mean(simulated_X) `

2.8502000000000001  

这是由于您可以在经验直方图中看到：它看起来非常像X的概率直方图。大约15％的模拟值是1，大约20％是2，依此类推，所以平均值非常接近2.85。   
两个直方图的相似性是因为您在Data8中看到的平均定律，我们将在此课程中正式建立 。  
现在我们有几种考虑期望的方法，让我们看看为什么它具有如此重要的意义。 我们将从直接使用定义开始计算一些期望。在后续章节中，我们将开发更强大的方法来计算和使用期望。  
### 唯一性  
这个小例子值得写出来因为它一直被使用。假设随机变量X实际上是一个常数c，即假设P（X = c）= 1。 然后X的分布将其所有质量放在单个值c上，并且E（X）=c⋅1= c。 我们只写E（c）= c。  
###伯努利和指标  
如果X服从伯努利（p）分布，那么P（X = 1）=p和P（X = 0）= 1-p。那么  

　　　　　　　　　　　　　　　E(X)=0⋅(1−p) + 1⋅p = p  　　
如上所述，零/一值随机变量是其他变量的构建块，它们被称为指标。  
假设A可以是任何事件，然后A的指标是随机变量I<sub>A</sub>，如果A发生则为1，如果A不发生则为0。这样I<sub>A</sub>就服从伯努利(P(A)) 分布，可以通过E(I<sub>A</sub>)=P(A)该式计算。因此，每个概率都是一种期望。我们将在后面的部分中大量使用它。  
```
X = [0, 1]    
qp = [0.75, 0.25]    
bern_1_3 = Table().values(x).probability(qp)    
Plot(bern_1_3, show_ev=True)    
plt.title('Bernoulli (0.25)')   
```  
![](https://i.imgur.com/8JE9Jq8.png)

### 整数区间的一致性  
设a和b为两个整数，使a <b。如果X在整数a，a + 1，a + 2，...，b上具有均匀分布，那么根据对称性，E(X)应该是在a和b中间位置的。这就是可以将概率直方图平衡的地方。所以有
　　　　　　　　　　　　　　$$ E(X) = \frac{a+b}{2} $$
例如，如果X在1,2，...，n上具有均匀分布，那么  
　　　　　　　　　　　　　　　　　　　　E(X)=(n+1)/2　　
这种情况的一个例子是，如果X是骰子的一个的斑点数，那么E(X)=3.5。  

如果X在0,1,2，...，n上是均匀的，那么E(X)=n/2。  
```
x = np.arange(10)
probs = 0.1*np.ones(10)
unif_10 = Table().values(x).probability(probs)
Plot(unif_10, show_ev=True)
plt.title('Uniform on Integers 0, 1, 2, ..., 9')
```
![](https://i.imgur.com/w0J2Qpb.png)

### 泊松分布

设X具有泊松分布(μ)，则  

　　　　　　　![](https://i.imgur.com/6vOMG96.png)

现在我们要对泊松分布的参数进行一个重要的新解释。我们之前看到它的接近形式，现在我们知道这也是分布的平衡点或期望，我们可以使用符号μ代表“均值”。  
```
k = np.arange(15)
poi_2_probs = stats.poisson.pmf(k, 2)
dist_poi_2 = Table().values(k).probability(poi_2_probs)
Plot(dist_poi_2, show_ev=True)
plt.title('Poisson (2)')
```
![](https://i.imgur.com/1Cn6hm9.png)
### 存在性
如果X有相当多的值，那么用于定义期望的总和是无限的，因此被限制为求部分和。但并非所有部分和的序列都是有限的，因此并非所有随机变量都有期望。实际上，当和是绝对收敛时，E（X）才被很好地定义：  
$$ E(X) = \sum_{\text{all }x} xP(X=x) ~~~~ \text{provided } \sum_{\text{all }x} |x|P(X=x) < \infty $$    
对于这个水平的课程来说，这有点技术性，在Pro140中，你几乎永远不必处理不存在的期望。请记住，期望并不总是有限的，甚至不是很明确。
这里有一个例子，你可以看到期望不能是有限的。 首先注意序列1 / 2n，n = 1,2,3，...是概率分布，即通过对几何系列求和得到1。  
　　　　　　![](https://i.imgur.com/8cXoicc.png)　　
现在假设随机变量Ｘ具有2,4,8,16 ...这些值，因此对于n=1,2,3，...，P（Ｘ=2n）= 1/ 2n。然后对于每个可能的值Ｘ，乘积xＰ（Ｘ= x）= 1。如果你试图添加无限多的1，唯一合理的答案是无限。　　
当分布具有“质量漂移至无穷大”的速率时，就会发生这种期望的问题，这时的速率在概率直方图水平轴方向的任何位置上无法达到平衡。  
### 8.2 可加性  
```
# HIDDEN
from datascience import *
from prob140 import *
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
%matplotlib inline
import math
from scipy import stats
from scipy import misc
```
在简单的情况下，通过插入定义来计算期望可以起作用，但通常它可能很麻烦或缺乏洞察力。 可以证实，计算期望的最有力结果不是定义。它看起来相当innocuous。  
### 可加性 
设X和Y是在相同概率空间上定义的两个随机变量，那么  
　　　　　　　　Ｅ(X+Y) = E(X) + E(Y)  
在我们更仔细地研究这个结果之前，请注意我们事先假设所有期望是存在的; 我们将在本课程中始终贯彻这个假设。  
现在注意到没有关于X和Y之间关系的假设。它们可以是相关或独立的。无论如何，总和的期望是期望的总和。 这使得结果强有力。  
从X + Y的定义和域空间的期望定义可以很容易地看出可加性。首先注意随机变量X + Y是由(X+Y)(ω)=X(ω)+Y(ω) for all ω∈Ω定义的函数。  
因此，“由概率加权的X + Y的值”可写为(X+Y)(ω)⋅P(ω)=X(ω)P(ω)+Y(ω)P(ω)。  
从两方面对所有ω∈Ω求和，以证明期望的可加性。  
通过归纳，可加性扩展到任何有限数量的随机变量。如果X1，X2，...，Xn是在相同概率空间上定义的随机变量，那么E(X1+X2+⋯+Xn)=E(X1)+E(X2)+⋯+E(Xn)。  
如果你试图找到一个期望，那么使用可加性的方法是将随机变量写成一个简单变量的总和，这些变量的期望是你知道的或者可以很容易地计算出来的。 本节的其余部分包含此技术的示例。  
### 样本总和
设X1，X2，...，Xn是服从均匀分布为μ的数字总体中随机抽取的样本，并且使样本总和为Sn=X1+X2+⋯+Xn。 然后无论是否抽取样品，每个Xi都具有与种群相同的分布。如果样品被抽取的话，这显然是正确的，正如我们在前面的章节中看到的那样，如果样品没有被抽取，那么就是对称的。  
因此和是否抽取样本是无关的，（待校对） E(Sn)=E(X1)+E(X2)+⋯+E(Xn)=nμ  
### 线性函数规则
设X为随机变量，期望为E（X），并且对于某一常数a，设Y = aX。 例如，当您将随机长度的单位从英寸更改为厘米时，则a = 2.54(注：1英寸(in)=2.54厘米(cm))。
